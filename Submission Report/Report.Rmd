---
title: "Bhuyan_Chuang_Martinez_CompetitionReport_S25"
author: "Mazhar Bhuyan"
date: "2025-04-25"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 7,
  fig.height = 4,
  tidy = FALSE,
  tidy.opts = list(width.cutoff = 80)
)
```


## GitHub Repository

[Project Repository](https://github.com/jessalynlc/BhuyanChuangMartinez_ENV797_TSA_ForecastCompetition_S25)

## Introduction

This report presents our forecasting approach for the ENV797 TSA Forecasting Competition. Our objective was to forecast daily electricity demand using time series models. We applied multiple models and compared their forecasting accuracy, aiming to outperform the benchmark STL+ETS model.

## Data Description

The dataset includes hourly electricity load, relative humidity, and temperature data from January 1, 2005 to December 31, 2010. These were aggregated into daily averages to facilitate daily forecasting.

```{r loading packages, echo=FALSE}
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)
library(zoo)
library(kableExtra)
library(readxl)
library(xts)
library(here)
```

```{r data import, echo=FALSE, warning=FALSE}
# Load data
load_data <- read_excel(here::here("Data", "load.xlsx"))
relative_humidity_data <- read_excel(here::here("Data", "relative_humidity.xlsx"))
temperature_data <- read_excel(here::here("Data", "temperature.xlsx"))

```

```{r data wrangling, echo=FALSE, warning=FALSE}
# Process load data
load_processed <- load_data %>%
  pivot_longer(cols = starts_with("h"), names_to = "hour", values_to = "electricity_demand") %>%
  mutate(hour = as.integer(sub("h", "", hour)), date = ymd(date)) %>%
  group_by(date) %>%
  summarise(daily_avg_load = mean(electricity_demand, na.rm = TRUE)) %>%
  ungroup()

# Process humidity data
humidity_processed <- relative_humidity_data %>%
  pivot_longer(cols = starts_with("rh"), names_to = "hour", values_to = "relative_humidity") %>%
  mutate(hour = as.integer(gsub("[^0-9]", "", hour)), date = ymd(date)) %>%
  group_by(date) %>%
  summarise(daily_avg_humidity = mean(relative_humidity, na.rm = TRUE)) %>%
  ungroup()

# Process temperature data
temp_processed <- temperature_data %>%
  pivot_longer(cols = starts_with("t"), names_to = "hour", values_to = "temperature") %>%
  mutate(hour = as.integer(gsub("[^0-9]", "", hour)), date = ymd(date)) %>%
  group_by(date) %>%
  summarise(daily_avg_temp = mean(temperature, na.rm = TRUE)) %>%
  ungroup()

# Merge all daily data
full_daily <- load_processed %>%
  inner_join(temp_processed, by = "date") %>%
  inner_join(humidity_processed, by = "date") %>%
  arrange(date)

head(full_daily)
```

We then created `msts` time series objects for both load and temperature, capturing weekly and annual seasonality.

```{r ts-objects, echo=TRUE}
library(forecast)
ts_electricity_daily <- msts(full_daily$daily_avg_load,
                             seasonal.periods = c(7, 365.25),
                             start = decimal_date(as.Date("2005-01-01")))

ts_temp_daily <- msts(full_daily$daily_avg_temp,
                      seasonal.periods = c(7, 365.25),
                      start = decimal_date(as.Date("2005-01-01")))
```

These were then split into training and test sets:

```{r ts-split, echo=TRUE}
ts_daily_train <- window(ts_electricity_daily, end = c(2009, 365))
ts_daily_test <- window(ts_electricity_daily, start = c(2010, 1), end = c(2010, 59))
temp_train <- as.numeric(window(ts_temp_daily, end = c(2009, 365)))
temp_test <- as.numeric(window(ts_temp_daily, start = c(2010, 1), end = c(2010, 59)))
horizon <- length(ts_daily_test)
```


The dataset includes hourly electricity load, relative humidity, and temperature data from January 1, 2005 to December 31, 2010. We aggregated the data into daily averages and created time series objects with weekly and annual seasonality. This structure allows our models to capture both short- and long-term patterns.

## Top 5 Forecasting Models

We tested a wide range of forecasting models and selected our top performers based on their ability to capture seasonal patterns, accommodate nonlinear relationships, and minimize forecasting error. The choice of models was guided by both theoretical understanding and empirical validation.

- **NNAR (Neural Network Autoregression)** was selected due to its strength in modeling nonlinear relationships. We combined NNAR with Fourier terms to capture seasonal components explicitly, especially those beyond the model's inherent capacity.

- **Fourier terms** (K=2,8), (K=2,12), and (K=3,18) allowed us to flexibly model multiple seasonal cycles (weekly and annual). These were added as exogenous regressors to improve model fit and forecasting accuracy.

- **Temperature** was tested as an additional regressor because electricity demand can be weather-sensitive. However, its inclusion did not universally enhance performance, as seen in Model 3.

- **TBATS** was included because it is specifically designed for complex seasonal patterns and automatically handles multiple seasonality, damping, and Box-Cox transformations.

- **ARIMA + NNAR Hybrid** was developed to combine strengths: ARIMA for linear trends and autocorrelation, and NNAR for learning residual nonlinear patterns. This model achieved the best performance across metrics.

The six models below represent the final selections used in our comparison and final forecast.

We evaluated several models and selected the following top five based on validation performance (Janâ€“Feb 2010):

1. Model 1: NNAR + Fourier (K=2,8)
2. Model 2: NNAR + Fourier (K=2,12) Baseline
3. Model 3: NNAR + Fourier (K=2,12) + Temp
4. Model 4: NNAR + Fourier (K=3,18)
5. Model 5: TBATS
6. Model 6: ARIMA + NNAR Hybrid

The forecasting horizon was set to 59 days, and each model was evaluated using RMSE, MAE, and MAPE.

## Modeling & Forecast Results

### Model 1: NNAR + Fourier (K = c(2,8))
```{r model1-nnar-k28, echo=TRUE}
horizon <- length(ts_daily_test)
NN_fit_k28 <- nnetar(ts_daily_train, p = 2, P = 2, xreg = fourier(ts_daily_train, K = c(2, 8)))
NN_for_k28 <- forecast(NN_fit_k28, h = horizon, xreg = fourier(ts_daily_train, K = c(2, 8), h = horizon))
autoplot(ts_daily_test) + autolayer(NN_for_k28, series = "Model 1: NNAR K=2,8")
accuracy(NN_for_k28, ts_daily_test)
```

### Model 2: NNAR + Fourier (K = c(2,12)) Baseline
```{r model2-nnar-k212, echo=TRUE}
NN_fit_k212_base <- nnetar(ts_daily_train, p = 2, P = 2, xreg = fourier(ts_daily_train, K = c(2, 12)))
NN_for_k212_base <- forecast(NN_fit_k212_base, h = horizon, xreg = fourier(ts_daily_train, K = c(2, 12), h = horizon))
autoplot(ts_daily_test) + autolayer(NN_for_k212_base, series = "Model 2: NNAR K=2,12")
accuracy(NN_for_k212_base, ts_daily_test)
```

### Model 3: NNAR + Fourier (K = c(2,12)) + Temperature
```{r model3-nnar-k212-temp, echo=TRUE}
xreg_train <- cbind(temp_train, fourier(ts_daily_train, K = c(2, 12)))
xreg_test  <- cbind(temp_test, fourier(ts_daily_train, K = c(2, 12), h = horizon))
NN_fit_k212 <- nnetar(ts_daily_train, p = 2, P = 2, xreg = xreg_train)
NN_for_k212 <- forecast(NN_fit_k212, h = horizon, xreg = xreg_test)
autoplot(ts_daily_test) + autolayer(NN_for_k212, series = "Model 3: NNAR + Temp")
accuracy(NN_for_k212, ts_daily_test)
```

### Model 4: NNAR + Fourier (K = c(3,18))
```{r model4-nnar-k318, echo=TRUE}
xreg_train <- fourier(ts_daily_train, K = c(3, 18))
xreg_test  <- fourier(ts_daily_train, K = c(3, 18), h = horizon)
NN_fit_k318 <- nnetar(ts_daily_train, p = 2, P = 2, xreg = xreg_train, size = 10, decay = 0.01, maxNWts = 2000)
NN_for_k318 <- forecast(NN_fit_k318, h = horizon, xreg = xreg_test)
autoplot(ts_daily_test) + autolayer(NN_for_k318, series = "Model 4: NNAR K=3,18")
accuracy(NN_for_k318, ts_daily_test)
```

### Model 5: TBATS
```{r model5-tbats, echo=TRUE}
TBATS_fit <- tbats(ts_daily_train)
TBATS_for <- forecast(TBATS_fit, h = horizon)
autoplot(ts_daily_test) + autolayer(TBATS_for, series = "Model 5: TBATS")
accuracy(TBATS_for, ts_daily_test)
```

### Model 6: ARIMA + NNAR Hybrid
```{r model6-hybrid, echo=TRUE}
ts_daily_train_ts <- ts(as.numeric(ts_daily_train), start = c(2005, 1), frequency = 365.25)
fit_arima <- auto.arima(ts_daily_train_ts, seasonal = TRUE)
fc_arima_test <- forecast(fit_arima, h = horizon)
res_arima <- residuals(fit_arima)
fit_nnar_resid <- nnetar(res_arima)
fc_nnar_resid_test <- forecast(fit_nnar_resid, h = horizon)
fc_hybrid_test <- fc_arima_test$mean + fc_nnar_resid_test$mean
fc_hybrid_ts <- ts(fc_hybrid_test, start = start(ts_daily_test), frequency = frequency(ts_daily_test))
autoplot(ts_daily_test) +
  autolayer(fc_arima_test$mean, series = "ARIMA") +
  autolayer(fc_hybrid_ts, series = "ARIMA + NNAR Hybrid")
accuracy_hybrid <- accuracy(fc_hybrid_ts, ts_daily_test)
accuracy_hybrid
```

### Model Comparison Table
```{r model-comparison, echo=TRUE}
comparison <- data.frame(
  Model = c("NNAR (2,8)", "NNAR (2,12)", "NNAR (2,12)+Temp", "NNAR (3,18)", "TBATS", "ARIMA+NNAR Hybrid"),
  RMSE = c(
    accuracy(NN_for_k28, ts_daily_test)[2, "RMSE"],
    accuracy(NN_for_k212_base, ts_daily_test)[2, "RMSE"],
    accuracy(NN_for_k212, ts_daily_test)[2, "RMSE"],
    accuracy(NN_for_k318, ts_daily_test)[2, "RMSE"],
    accuracy(TBATS_for, ts_daily_test)[2, "RMSE"],
    accuracy_hybrid["RMSE"]
  ),
  MAE = c(
    accuracy(NN_for_k28, ts_daily_test)[2, "MAE"],
    accuracy(NN_for_k212_base, ts_daily_test)[2, "MAE"],
    accuracy(NN_for_k212, ts_daily_test)[2, "MAE"],
    accuracy(NN_for_k318, ts_daily_test)[2, "MAE"],
    accuracy(TBATS_for, ts_daily_test)[2, "MAE"],
    accuracy_hybrid["MAE"]
  ),
  MAPE = c(
    accuracy(NN_for_k28, ts_daily_test)[2, "MAPE"],
    accuracy(NN_for_k212_base, ts_daily_test)[2, "MAPE"],
    accuracy(NN_for_k212, ts_daily_test)[2, "MAPE"],
    accuracy(NN_for_k318, ts_daily_test)[2, "MAPE"],
    accuracy(TBATS_for, ts_daily_test)[2, "MAPE"],
    accuracy_hybrid["MAPE"]
  )
)
kable(comparison, caption = "Model Comparison on Validation Set")
```

(Existing chunks continue...)

## Final Forecast for 2011 (All Top Models)

The following forecasts were generated for January 1 to February 28, 2011 using the full training dataset (2005â€“2010).

- Model 3: NNAR + Fourier (K = c(2,12)) + Temp
- Model 4: NNAR + Fourier (K = c(3,18))
- Model 6: ARIMA + NNAR Hybrid

Each model was retrained on the full dataset. Forecasts were saved and submitted in CSV format.

## Conclusion

Each of the top-performing models had unique strengths:

- **Model 1 (NNAR + Fourier (K=2,8))** performed well with low complexity. It captured both weekly and annual seasonality with fewer harmonics, making it computationally efficient.

- **Model 2 (NNAR + Fourier (K=2,12))** served as our baseline and slightly improved accuracy by incorporating more seasonal harmonics, especially useful for longer patterns beyond weekly variation.

- **Model 3 (NNAR + Fourier (K=2,12) + Temp)** attempted to improve accuracy with temperature as an exogenous regressor. However, the added complexity did not always yield better results, suggesting limited marginal value from temperature in this setup.

- **Model 4 (NNAR + Fourier (K=3,18))** improved flexibility by including more harmonics and regularization (decay), enabling better capture of complex seasonality patterns at the cost of higher training time.

- **Model 5 (TBATS)** automatically handled multiple seasonality and damped trends, offering good accuracy with minimal tuning. However, it was outperformed by the hybrid model.

- **Model 6 (ARIMA + NNAR Hybrid)** outperformed all other models by combining linear and nonlinear components. The ARIMA model captured trend and autocorrelation, while the NNAR learned residual nonlinearities. This hybridization proved especially effective in reducing bias and improving generalization.


This report reflects our systematic approach to model selection, validation, and forecasting. We used reproducible workflows and version control via GitHub throughout the project.

## Acknowledgment of AI Assistance

ChatGPT was used for troubleshooting code syntax. All models and forecasts were executed and validated locally in RStudio by the authors.
